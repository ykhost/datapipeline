[2022-03-15 19:28:03,006] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: twitter_dag.transform_twiiter_aluraonline 2022-03-11T09:00:00+00:00 [queued]>
[2022-03-15 19:28:03,011] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: twitter_dag.transform_twiiter_aluraonline 2022-03-11T09:00:00+00:00 [queued]>
[2022-03-15 19:28:03,011] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-03-15 19:28:03,011] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-03-15 19:28:03,011] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-03-15 19:28:03,017] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_twiiter_aluraonline> on 2022-03-11T09:00:00+00:00
[2022-03-15 19:28:03,018] {standard_task_runner.py:54} INFO - Started process 31854 to run task
[2022-03-15 19:28:03,047] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'twitter_dag', 'transform_twiiter_aluraonline', '2022-03-11T09:00:00+00:00', '--job_id', '74', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/twitter_dag.py', '--cfg_path', '/tmp/tmpeln30m_6']
[2022-03-15 19:28:03,047] {standard_task_runner.py:78} INFO - Job 74: Subtask transform_twiiter_aluraonline
[2022-03-15 19:28:03,062] {logging_mixin.py:112} INFO - Running <TaskInstance: twitter_dag.transform_twiiter_aluraonline 2022-03-11T09:00:00+00:00 [running]> on host 1a222f561522
[2022-03-15 19:28:03,090] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-03-15 19:28:03,091] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /run/pypsark/spark-3.1.2-bin-hadoop2.7/bin/spark-submit --master local --name twitter_transformation /run/datapipeline/spark/transformation.py --src /run/datapipeline/datalake/bronze/twitter_bbb22/extract_date=2022-03-11 --dest /run/datapipeline/datalake/silver/twitter_bbb22/ --process-date 2022-03-11
[2022-03-15 19:28:04,137] {spark_submit_hook.py:479} INFO - WARNING: An illegal reflective access operation has occurred
[2022-03-15 19:28:04,137] {spark_submit_hook.py:479} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/run/pypsark/spark-3.1.2-bin-hadoop2.7/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-03-15 19:28:04,137] {spark_submit_hook.py:479} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-03-15 19:28:04,137] {spark_submit_hook.py:479} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-03-15 19:28:04,137] {spark_submit_hook.py:479} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-03-15 19:28:04,610] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-03-15 19:28:05,255] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-03-15 19:28:05,264] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO SparkContext: Running Spark version 3.1.2
[2022-03-15 19:28:05,293] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO ResourceUtils: ==============================================================
[2022-03-15 19:28:05,293] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-03-15 19:28:05,293] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO ResourceUtils: ==============================================================
[2022-03-15 19:28:05,294] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO SparkContext: Submitted application: twitter_transformation
[2022-03-15 19:28:05,319] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-03-15 19:28:05,339] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO ResourceProfile: Limiting resource is cpu
[2022-03-15 19:28:05,340] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-03-15 19:28:05,370] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO SecurityManager: Changing view acls to: root
[2022-03-15 19:28:05,371] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO SecurityManager: Changing modify acls to: root
[2022-03-15 19:28:05,371] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO SecurityManager: Changing view acls groups to:
[2022-03-15 19:28:05,371] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO SecurityManager: Changing modify acls groups to:
[2022-03-15 19:28:05,371] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[2022-03-15 19:28:05,521] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO Utils: Successfully started service 'sparkDriver' on port 43323.
[2022-03-15 19:28:05,543] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO SparkEnv: Registering MapOutputTracker
[2022-03-15 19:28:05,568] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO SparkEnv: Registering BlockManagerMaster
[2022-03-15 19:28:05,581] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-03-15 19:28:05,582] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-03-15 19:28:05,585] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-03-15 19:28:05,595] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7c058531-fec5-4716-87ec-b91441b92e57
[2022-03-15 19:28:05,611] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2022-03-15 19:28:05,623] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-03-15 19:28:05,761] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-03-15 19:28:05,803] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://1a222f561522:4040
[2022-03-15 19:28:05,945] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO Executor: Starting executor ID driver on host 1a222f561522
[2022-03-15 19:28:05,962] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37595.
[2022-03-15 19:28:05,962] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO NettyBlockTransferService: Server created on 1a222f561522:37595
[2022-03-15 19:28:05,963] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-03-15 19:28:05,974] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1a222f561522, 37595, None)
[2022-03-15 19:28:05,983] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO BlockManagerMasterEndpoint: Registering block manager 1a222f561522:37595 with 434.4 MiB RAM, BlockManagerId(driver, 1a222f561522, 37595, None)
[2022-03-15 19:28:05,988] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1a222f561522, 37595, None)
[2022-03-15 19:28:05,989] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1a222f561522, 37595, None)
[2022-03-15 19:28:06,318] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/run/datapipeline/spark-warehouse').
[2022-03-15 19:28:06,318] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:06 INFO SharedState: Warehouse path is 'file:/run/datapipeline/spark-warehouse'.
[2022-03-15 19:28:06,867] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:06 INFO InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.
[2022-03-15 19:28:06,918] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:06 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-03-15 19:28:08,134] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO FileSourceStrategy: Pushed Filters:
[2022-03-15 19:28:08,135] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO FileSourceStrategy: Post-Scan Filters:
[2022-03-15 19:28:08,137] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-03-15 19:28:08,323] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 171.5 KiB, free 434.2 MiB)
[2022-03-15 19:28:08,363] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 434.2 MiB)
[2022-03-15 19:28:08,365] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1a222f561522:37595 (size: 24.0 KiB, free: 434.4 MiB)
[2022-03-15 19:28:08,368] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-03-15 19:28:08,373] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207111 bytes, open cost is considered as scanning 4194304 bytes.
[2022-03-15 19:28:08,482] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-03-15 19:28:08,494] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-03-15 19:28:08,494] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-03-15 19:28:08,495] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO DAGScheduler: Parents of final stage: List()
[2022-03-15 19:28:08,496] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO DAGScheduler: Missing parents: List()
[2022-03-15 19:28:08,500] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-03-15 19:28:08,569] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 434.2 MiB)
[2022-03-15 19:28:08,573] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2022-03-15 19:28:08,574] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 1a222f561522:37595 (size: 6.3 KiB, free: 434.4 MiB)
[2022-03-15 19:28:08,575] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1388
[2022-03-15 19:28:08,585] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-03-15 19:28:08,586] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-03-15 19:28:08,625] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (1a222f561522, executor driver, partition 0, PROCESS_LOCAL, 4923 bytes) taskResourceAssignments Map()
[2022-03-15 19:28:08,638] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-03-15 19:28:08,730] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO FileScanRDD: Reading File path: file:///run/datapipeline/datalake/bronze/twitter_bbb22/extract_date=2022-03-11/bbb22_20220311.json, range: 0-12807, partition values: [empty row]
[2022-03-15 19:28:08,959] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:08 INFO CodeGenerator: Code generated in 138.150487 ms
[2022-03-15 19:28:09,002] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2858 bytes result sent to driver
[2022-03-15 19:28:09,009] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 391 ms on 1a222f561522 (executor driver) (1/1)
[2022-03-15 19:28:09,011] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-03-15 19:28:09,015] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0.506 s
[2022-03-15 19:28:09,018] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-03-15 19:28:09,018] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-03-15 19:28:09,020] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0.537929 s
[2022-03-15 19:28:09,348] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2022-03-15 19:28:09,349] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2022-03-15 19:28:09,349] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,id:string,in_reply_to_user_id:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2022-03-15 19:28:09,404] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 1a222f561522:37595 in memory (size: 24.0 KiB, free: 434.4 MiB)
[2022-03-15 19:28:09,410] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 1a222f561522:37595 in memory (size: 6.3 KiB, free: 434.4 MiB)
[2022-03-15 19:28:09,425] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-03-15 19:28:09,426] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-03-15 19:28:09,479] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO CodeGenerator: Code generated in 25.450047 ms
[2022-03-15 19:28:09,516] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO CodeGenerator: Code generated in 22.958385 ms
[2022-03-15 19:28:09,521] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 171.4 KiB, free 434.2 MiB)
[2022-03-15 19:28:09,529] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 434.2 MiB)
[2022-03-15 19:28:09,530] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 1a222f561522:37595 (size: 23.9 KiB, free: 434.4 MiB)
[2022-03-15 19:28:09,531] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-03-15 19:28:09,533] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207111 bytes, open cost is considered as scanning 4194304 bytes.
[2022-03-15 19:28:09,595] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-03-15 19:28:09,597] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-03-15 19:28:09,597] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-03-15 19:28:09,597] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: Parents of final stage: List()
[2022-03-15 19:28:09,597] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: Missing parents: List()
[2022-03-15 19:28:09,598] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[13] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-03-15 19:28:09,630] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 167.6 KiB, free 434.0 MiB)
[2022-03-15 19:28:09,635] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 59.6 KiB, free 434.0 MiB)
[2022-03-15 19:28:09,636] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 1a222f561522:37595 (size: 59.6 KiB, free: 434.3 MiB)
[2022-03-15 19:28:09,636] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1388
[2022-03-15 19:28:09,637] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[13] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-03-15 19:28:09,637] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-03-15 19:28:09,640] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (1a222f561522, executor driver, partition 0, PROCESS_LOCAL, 5152 bytes) taskResourceAssignments Map()
[2022-03-15 19:28:09,640] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-03-15 19:28:09,688] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-03-15 19:28:09,689] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-03-15 19:28:09,741] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO CodeGenerator: Code generated in 20.436549 ms
[2022-03-15 19:28:09,743] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO FileScanRDD: Reading File path: file:///run/datapipeline/datalake/bronze/twitter_bbb22/extract_date=2022-03-11/bbb22_20220311.json, range: 0-12807, partition values: [empty row]
[2022-03-15 19:28:09,764] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO CodeGenerator: Code generated in 17.699137 ms
[2022-03-15 19:28:09,783] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO CodeGenerator: Code generated in 4.781454 ms
[2022-03-15 19:28:09,815] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO FileOutputCommitter: Saved output of task 'attempt_202203151928091449558433995289836_0001_m_000000_1' to file:/run/datapipeline/datalake/silver/twitter_bbb22/tweet/process_date=2022-03-11/_temporary/0/task_202203151928091449558433995289836_0001_m_000000
[2022-03-15 19:28:09,816] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO SparkHadoopMapRedUtil: attempt_202203151928091449558433995289836_0001_m_000000_1: Committed
[2022-03-15 19:28:09,819] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2022-03-15 19:28:09,821] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 183 ms on 1a222f561522 (executor driver) (1/1)
[2022-03-15 19:28:09,821] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-03-15 19:28:09,822] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.222 s
[2022-03-15 19:28:09,822] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-03-15 19:28:09,822] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-03-15 19:28:09,822] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.227039 s
[2022-03-15 19:28:09,834] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO FileFormatWriter: Write Job 7b5d01c4-ecc5-4c6c-87d7-8ae0e74df336 committed.
[2022-03-15 19:28:09,837] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO FileFormatWriter: Finished processing stats for write job 7b5d01c4-ecc5-4c6c-87d7-8ae0e74df336.
[2022-03-15 19:28:09,879] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(includes)
[2022-03-15 19:28:09,879] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(includes#8),(size(includes#8.users, true) > 0),isnotnull(includes#8.users)
[2022-03-15 19:28:09,879] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2022-03-15 19:28:09,892] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-03-15 19:28:09,892] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-03-15 19:28:09,905] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO CodeGenerator: Code generated in 8.797617 ms
[2022-03-15 19:28:09,922] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO CodeGenerator: Code generated in 12.448394 ms
[2022-03-15 19:28:09,925] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 171.4 KiB, free 433.8 MiB)
[2022-03-15 19:28:09,935] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 433.8 MiB)
[2022-03-15 19:28:09,936] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 1a222f561522:37595 (size: 23.9 KiB, free: 434.3 MiB)
[2022-03-15 19:28:09,937] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-03-15 19:28:09,937] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207111 bytes, open cost is considered as scanning 4194304 bytes.
[2022-03-15 19:28:09,970] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-03-15 19:28:09,971] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-03-15 19:28:09,971] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-03-15 19:28:09,971] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: Parents of final stage: List()
[2022-03-15 19:28:09,972] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: Missing parents: List()
[2022-03-15 19:28:09,974] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[25] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-03-15 19:28:09,989] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 163.9 KiB, free 433.6 MiB)
[2022-03-15 19:28:09,994] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 58.8 KiB, free 433.6 MiB)
[2022-03-15 19:28:09,995] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 1a222f561522:37595 (size: 58.8 KiB, free: 434.2 MiB)
[2022-03-15 19:28:09,995] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1388
[2022-03-15 19:28:09,996] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[25] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-03-15 19:28:09,996] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-03-15 19:28:09,998] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (1a222f561522, executor driver, partition 0, PROCESS_LOCAL, 5152 bytes) taskResourceAssignments Map()
[2022-03-15 19:28:10,002] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:09 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-03-15 19:28:10,013] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-03-15 19:28:10,013] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-03-15 19:28:10,040] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO CodeGenerator: Code generated in 10.176737 ms
[2022-03-15 19:28:10,043] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO FileScanRDD: Reading File path: file:///run/datapipeline/datalake/bronze/twitter_bbb22/extract_date=2022-03-11/bbb22_20220311.json, range: 0-12807, partition values: [empty row]
[2022-03-15 19:28:10,057] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO CodeGenerator: Code generated in 11.344756 ms
[2022-03-15 19:28:10,070] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO CodeGenerator: Code generated in 3.561574 ms
[2022-03-15 19:28:10,071] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO FileOutputCommitter: Saved output of task 'attempt_202203151928091702594331694648883_0002_m_000000_2' to file:/run/datapipeline/datalake/silver/twitter_bbb22/user/process_date=2022-03-11/_temporary/0/task_202203151928091702594331694648883_0002_m_000000
[2022-03-15 19:28:10,071] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO SparkHadoopMapRedUtil: attempt_202203151928091702594331694648883_0002_m_000000_2: Committed
[2022-03-15 19:28:10,073] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2858 bytes result sent to driver
[2022-03-15 19:28:10,074] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 77 ms on 1a222f561522 (executor driver) (1/1)
[2022-03-15 19:28:10,074] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-03-15 19:28:10,076] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.101 s
[2022-03-15 19:28:10,076] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-03-15 19:28:10,076] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-03-15 19:28:10,076] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.107656 s
[2022-03-15 19:28:10,085] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO FileFormatWriter: Write Job 155e0d42-2b80-4300-84e6-d38793d5064c committed.
[2022-03-15 19:28:10,085] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO FileFormatWriter: Finished processing stats for write job 155e0d42-2b80-4300-84e6-d38793d5064c.
[2022-03-15 19:28:10,130] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO SparkUI: Stopped Spark web UI at http://1a222f561522:4040
[2022-03-15 19:28:10,144] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-03-15 19:28:10,154] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO MemoryStore: MemoryStore cleared
[2022-03-15 19:28:10,154] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO BlockManager: BlockManager stopped
[2022-03-15 19:28:10,157] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-03-15 19:28:10,159] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-03-15 19:28:10,161] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO SparkContext: Successfully stopped SparkContext
[2022-03-15 19:28:10,163] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO ShutdownHookManager: Shutdown hook called
[2022-03-15 19:28:10,164] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-1217701a-60eb-4e6e-9179-98b3d09dd899
[2022-03-15 19:28:10,165] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-bee15ab8-e796-48c6-a9ee-41822e753b21
[2022-03-15 19:28:10,167] {spark_submit_hook.py:479} INFO - 22/03/15 19:28:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-bee15ab8-e796-48c6-a9ee-41822e753b21/pyspark-0b32b5f7-b59e-4928-813a-ec0a984b497a
[2022-03-15 19:28:10,204] {taskinstance.py:1057} INFO - Marking task as SUCCESS.dag_id=twitter_dag, task_id=transform_twiiter_aluraonline, execution_date=20220311T090000, start_date=20220315T192803, end_date=20220315T192810
[2022-03-15 19:28:13,003] {local_task_job.py:102} INFO - Task exited with return code 0
[2022-03-15 21:28:47,601] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: twitter_dag.transform_twiiter_aluraonline 2022-03-11T09:00:00+00:00 [queued]>
[2022-03-15 21:28:47,607] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: twitter_dag.transform_twiiter_aluraonline 2022-03-11T09:00:00+00:00 [queued]>
[2022-03-15 21:28:47,607] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-03-15 21:28:47,607] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-03-15 21:28:47,607] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-03-15 21:28:47,613] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_twiiter_aluraonline> on 2022-03-11T09:00:00+00:00
[2022-03-15 21:28:47,615] {standard_task_runner.py:54} INFO - Started process 5467 to run task
[2022-03-15 21:28:47,645] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'twitter_dag', 'transform_twiiter_aluraonline', '2022-03-11T09:00:00+00:00', '--job_id', '75', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/twitter_dag.py', '--cfg_path', '/tmp/tmpp57l1oro']
[2022-03-15 21:28:47,645] {standard_task_runner.py:78} INFO - Job 75: Subtask transform_twiiter_aluraonline
[2022-03-15 21:28:47,661] {logging_mixin.py:112} INFO - Running <TaskInstance: twitter_dag.transform_twiiter_aluraonline 2022-03-11T09:00:00+00:00 [running]> on host 1a222f561522
[2022-03-15 21:28:47,692] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-03-15 21:28:47,693] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /run/pypsark/spark-3.1.2-bin-hadoop2.7/bin/spark-submit --master local --name twitter_transformation /run/datapipeline/spark/transformation.py --src /run/datapipeline/datalake/bronze/twitter_bbb22/extract_date=2022-03-11 --dest /run/datapipeline/datalake/silver/twitter_bbb22/ --process-date 2022-03-11
[2022-03-15 21:28:48,622] {spark_submit_hook.py:479} INFO - WARNING: An illegal reflective access operation has occurred
[2022-03-15 21:28:48,622] {spark_submit_hook.py:479} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/run/pypsark/spark-3.1.2-bin-hadoop2.7/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-03-15 21:28:48,622] {spark_submit_hook.py:479} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-03-15 21:28:48,622] {spark_submit_hook.py:479} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-03-15 21:28:48,622] {spark_submit_hook.py:479} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-03-15 21:28:48,984] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-03-15 21:28:49,492] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-03-15 21:28:49,499] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO SparkContext: Running Spark version 3.1.2
[2022-03-15 21:28:49,525] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO ResourceUtils: ==============================================================
[2022-03-15 21:28:49,526] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-03-15 21:28:49,526] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO ResourceUtils: ==============================================================
[2022-03-15 21:28:49,526] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO SparkContext: Submitted application: twitter_transformation
[2022-03-15 21:28:49,551] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-03-15 21:28:49,565] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO ResourceProfile: Limiting resource is cpu
[2022-03-15 21:28:49,565] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-03-15 21:28:49,594] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO SecurityManager: Changing view acls to: root
[2022-03-15 21:28:49,594] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO SecurityManager: Changing modify acls to: root
[2022-03-15 21:28:49,594] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO SecurityManager: Changing view acls groups to:
[2022-03-15 21:28:49,594] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO SecurityManager: Changing modify acls groups to:
[2022-03-15 21:28:49,595] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[2022-03-15 21:28:49,740] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO Utils: Successfully started service 'sparkDriver' on port 36479.
[2022-03-15 21:28:49,760] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO SparkEnv: Registering MapOutputTracker
[2022-03-15 21:28:49,781] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO SparkEnv: Registering BlockManagerMaster
[2022-03-15 21:28:49,794] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-03-15 21:28:49,794] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-03-15 21:28:49,798] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-03-15 21:28:49,807] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-86fae534-9fae-4423-9c95-ccba2c5209b4
[2022-03-15 21:28:49,823] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2022-03-15 21:28:49,835] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-03-15 21:28:49,976] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2022-03-15 21:28:49,981] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:49 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2022-03-15 21:28:50,020] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://1a222f561522:4041
[2022-03-15 21:28:50,159] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:50 INFO Executor: Starting executor ID driver on host 1a222f561522
[2022-03-15 21:28:50,178] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36437.
[2022-03-15 21:28:50,179] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:50 INFO NettyBlockTransferService: Server created on 1a222f561522:36437
[2022-03-15 21:28:50,180] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-03-15 21:28:50,183] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1a222f561522, 36437, None)
[2022-03-15 21:28:50,187] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:50 INFO BlockManagerMasterEndpoint: Registering block manager 1a222f561522:36437 with 434.4 MiB RAM, BlockManagerId(driver, 1a222f561522, 36437, None)
[2022-03-15 21:28:50,188] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1a222f561522, 36437, None)
[2022-03-15 21:28:50,189] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1a222f561522, 36437, None)
[2022-03-15 21:28:50,518] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/run/datapipeline/spark-warehouse').
[2022-03-15 21:28:50,518] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:50 INFO SharedState: Warehouse path is 'file:/run/datapipeline/spark-warehouse'.
[2022-03-15 21:28:51,049] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:51 INFO InMemoryFileIndex: It took 31 ms to list leaf files for 1 paths.
[2022-03-15 21:28:51,094] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:51 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-03-15 21:28:52,283] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO FileSourceStrategy: Pushed Filters:
[2022-03-15 21:28:52,283] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO FileSourceStrategy: Post-Scan Filters:
[2022-03-15 21:28:52,285] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-03-15 21:28:52,467] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 171.5 KiB, free 434.2 MiB)
[2022-03-15 21:28:52,508] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 434.2 MiB)
[2022-03-15 21:28:52,511] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1a222f561522:36437 (size: 24.0 KiB, free: 434.4 MiB)
[2022-03-15 21:28:52,513] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-03-15 21:28:52,518] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207153 bytes, open cost is considered as scanning 4194304 bytes.
[2022-03-15 21:28:52,630] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-03-15 21:28:52,641] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-03-15 21:28:52,641] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-03-15 21:28:52,641] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO DAGScheduler: Parents of final stage: List()
[2022-03-15 21:28:52,642] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO DAGScheduler: Missing parents: List()
[2022-03-15 21:28:52,645] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-03-15 21:28:52,702] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 434.2 MiB)
[2022-03-15 21:28:52,707] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2022-03-15 21:28:52,708] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 1a222f561522:36437 (size: 6.3 KiB, free: 434.4 MiB)
[2022-03-15 21:28:52,709] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1388
[2022-03-15 21:28:52,718] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-03-15 21:28:52,719] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-03-15 21:28:52,755] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (1a222f561522, executor driver, partition 0, PROCESS_LOCAL, 4923 bytes) taskResourceAssignments Map()
[2022-03-15 21:28:52,765] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-03-15 21:28:52,861] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:52 INFO FileScanRDD: Reading File path: file:///run/datapipeline/datalake/bronze/twitter_bbb22/extract_date=2022-03-11/bbb22_20220311.json, range: 0-12849, partition values: [empty row]
[2022-03-15 21:28:53,078] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO CodeGenerator: Code generated in 132.93354 ms
[2022-03-15 21:28:53,119] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2858 bytes result sent to driver
[2022-03-15 21:28:53,126] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 377 ms on 1a222f561522 (executor driver) (1/1)
[2022-03-15 21:28:53,128] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-03-15 21:28:53,132] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0.478 s
[2022-03-15 21:28:53,135] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-03-15 21:28:53,135] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-03-15 21:28:53,137] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0.507027 s
[2022-03-15 21:28:53,451] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2022-03-15 21:28:53,452] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2022-03-15 21:28:53,452] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,id:string,in_reply_to_user_id:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2022-03-15 21:28:53,499] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-03-15 21:28:53,500] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-03-15 21:28:53,528] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 1a222f561522:36437 in memory (size: 24.0 KiB, free: 434.4 MiB)
[2022-03-15 21:28:53,535] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 1a222f561522:36437 in memory (size: 6.3 KiB, free: 434.4 MiB)
[2022-03-15 21:28:53,584] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO CodeGenerator: Code generated in 33.194547 ms
[2022-03-15 21:28:53,622] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO CodeGenerator: Code generated in 24.000646 ms
[2022-03-15 21:28:53,628] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 171.4 KiB, free 434.2 MiB)
[2022-03-15 21:28:53,638] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 434.2 MiB)
[2022-03-15 21:28:53,639] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 1a222f561522:36437 (size: 23.9 KiB, free: 434.4 MiB)
[2022-03-15 21:28:53,640] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-03-15 21:28:53,643] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207153 bytes, open cost is considered as scanning 4194304 bytes.
[2022-03-15 21:28:53,716] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-03-15 21:28:53,718] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-03-15 21:28:53,718] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-03-15 21:28:53,718] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO DAGScheduler: Parents of final stage: List()
[2022-03-15 21:28:53,718] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO DAGScheduler: Missing parents: List()
[2022-03-15 21:28:53,719] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[13] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-03-15 21:28:53,755] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 167.6 KiB, free 434.0 MiB)
[2022-03-15 21:28:53,759] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 59.6 KiB, free 434.0 MiB)
[2022-03-15 21:28:53,759] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 1a222f561522:36437 (size: 59.6 KiB, free: 434.3 MiB)
[2022-03-15 21:28:53,760] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1388
[2022-03-15 21:28:53,761] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[13] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-03-15 21:28:53,761] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-03-15 21:28:53,764] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (1a222f561522, executor driver, partition 0, PROCESS_LOCAL, 5152 bytes) taskResourceAssignments Map()
[2022-03-15 21:28:53,765] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-03-15 21:28:53,816] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-03-15 21:28:53,817] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-03-15 21:28:53,868] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO CodeGenerator: Code generated in 19.193699 ms
[2022-03-15 21:28:53,871] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO FileScanRDD: Reading File path: file:///run/datapipeline/datalake/bronze/twitter_bbb22/extract_date=2022-03-11/bbb22_20220311.json, range: 0-12849, partition values: [empty row]
[2022-03-15 21:28:53,891] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO CodeGenerator: Code generated in 16.753404 ms
[2022-03-15 21:28:53,911] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO CodeGenerator: Code generated in 4.522215 ms
[2022-03-15 21:28:53,948] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO FileOutputCommitter: Saved output of task 'attempt_202203152128534086328661939761286_0001_m_000000_1' to file:/run/datapipeline/datalake/silver/twitter_bbb22/tweet/process_date=2022-03-11/_temporary/0/task_202203152128534086328661939761286_0001_m_000000
[2022-03-15 21:28:53,949] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO SparkHadoopMapRedUtil: attempt_202203152128534086328661939761286_0001_m_000000_1: Committed
[2022-03-15 21:28:53,953] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2022-03-15 21:28:53,955] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 193 ms on 1a222f561522 (executor driver) (1/1)
[2022-03-15 21:28:53,955] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-03-15 21:28:53,956] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.236 s
[2022-03-15 21:28:53,956] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-03-15 21:28:53,956] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-03-15 21:28:53,957] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.240478 s
[2022-03-15 21:28:53,970] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO FileFormatWriter: Write Job 40ef9105-544e-4020-a074-eabac1727470 committed.
[2022-03-15 21:28:53,973] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:53 INFO FileFormatWriter: Finished processing stats for write job 40ef9105-544e-4020-a074-eabac1727470.
[2022-03-15 21:28:54,021] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO FileSourceStrategy: Pushed Filters: IsNotNull(includes)
[2022-03-15 21:28:54,021] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(includes#8),(size(includes#8.users, true) > 0),isnotnull(includes#8.users)
[2022-03-15 21:28:54,022] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2022-03-15 21:28:54,032] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-03-15 21:28:54,033] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-03-15 21:28:54,048] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO CodeGenerator: Code generated in 10.171528 ms
[2022-03-15 21:28:54,065] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO CodeGenerator: Code generated in 12.566395 ms
[2022-03-15 21:28:54,068] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 171.4 KiB, free 433.8 MiB)
[2022-03-15 21:28:54,078] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 433.8 MiB)
[2022-03-15 21:28:54,079] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 1a222f561522:36437 (size: 23.9 KiB, free: 434.3 MiB)
[2022-03-15 21:28:54,079] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-03-15 21:28:54,080] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4207153 bytes, open cost is considered as scanning 4194304 bytes.
[2022-03-15 21:28:54,107] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-03-15 21:28:54,108] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-03-15 21:28:54,108] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-03-15 21:28:54,108] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO DAGScheduler: Parents of final stage: List()
[2022-03-15 21:28:54,109] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO DAGScheduler: Missing parents: List()
[2022-03-15 21:28:54,110] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[25] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-03-15 21:28:54,129] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 163.9 KiB, free 433.6 MiB)
[2022-03-15 21:28:54,133] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 58.8 KiB, free 433.6 MiB)
[2022-03-15 21:28:54,134] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 1a222f561522:36437 (size: 58.8 KiB, free: 434.2 MiB)
[2022-03-15 21:28:54,135] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1388
[2022-03-15 21:28:54,136] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[25] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-03-15 21:28:54,136] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-03-15 21:28:54,137] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (1a222f561522, executor driver, partition 0, PROCESS_LOCAL, 5152 bytes) taskResourceAssignments Map()
[2022-03-15 21:28:54,138] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-03-15 21:28:54,154] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-03-15 21:28:54,154] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-03-15 21:28:54,187] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO CodeGenerator: Code generated in 9.531222 ms
[2022-03-15 21:28:54,189] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO FileScanRDD: Reading File path: file:///run/datapipeline/datalake/bronze/twitter_bbb22/extract_date=2022-03-11/bbb22_20220311.json, range: 0-12849, partition values: [empty row]
[2022-03-15 21:28:54,203] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO CodeGenerator: Code generated in 11.794907 ms
[2022-03-15 21:28:54,210] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO CodeGenerator: Code generated in 4.581889 ms
[2022-03-15 21:28:54,222] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO FileOutputCommitter: Saved output of task 'attempt_202203152128543543969932528910293_0002_m_000000_2' to file:/run/datapipeline/datalake/silver/twitter_bbb22/user/process_date=2022-03-11/_temporary/0/task_202203152128543543969932528910293_0002_m_000000
[2022-03-15 21:28:54,222] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO SparkHadoopMapRedUtil: attempt_202203152128543543969932528910293_0002_m_000000_2: Committed
[2022-03-15 21:28:54,224] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2858 bytes result sent to driver
[2022-03-15 21:28:54,225] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 88 ms on 1a222f561522 (executor driver) (1/1)
[2022-03-15 21:28:54,226] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-03-15 21:28:54,227] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.115 s
[2022-03-15 21:28:54,227] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-03-15 21:28:54,227] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-03-15 21:28:54,227] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.120542 s
[2022-03-15 21:28:54,245] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO FileFormatWriter: Write Job f2277160-251e-463c-b958-af6d0fcbdc46 committed.
[2022-03-15 21:28:54,246] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO FileFormatWriter: Finished processing stats for write job f2277160-251e-463c-b958-af6d0fcbdc46.
[2022-03-15 21:28:54,301] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO SparkUI: Stopped Spark web UI at http://1a222f561522:4041
[2022-03-15 21:28:54,323] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-03-15 21:28:54,332] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO MemoryStore: MemoryStore cleared
[2022-03-15 21:28:54,332] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO BlockManager: BlockManager stopped
[2022-03-15 21:28:54,336] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-03-15 21:28:54,338] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-03-15 21:28:54,342] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO SparkContext: Successfully stopped SparkContext
[2022-03-15 21:28:54,344] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO ShutdownHookManager: Shutdown hook called
[2022-03-15 21:28:54,344] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-def3343c-2906-408a-ab1f-d8177c4c3418
[2022-03-15 21:28:54,348] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-def3343c-2906-408a-ab1f-d8177c4c3418/pyspark-7a0b41d6-3bb3-45f7-bf78-3e2c74a1277c
[2022-03-15 21:28:54,351] {spark_submit_hook.py:479} INFO - 22/03/15 21:28:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-676b393b-f5e6-4cb7-b03d-6ae40f85c1a4
[2022-03-15 21:28:54,388] {taskinstance.py:1057} INFO - Marking task as SUCCESS.dag_id=twitter_dag, task_id=transform_twiiter_aluraonline, execution_date=20220311T090000, start_date=20220315T212847, end_date=20220315T212854
[2022-03-15 21:28:57,602] {local_task_job.py:102} INFO - Task exited with return code 0
